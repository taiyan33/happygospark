{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "\n",
    "a = [1,2,3,'qoo', 4]\n",
    "a = [1,3,5,7,9]\n",
    "b = [2,4,6,8,10]\n",
    "\n",
    "res = []\n",
    "for i in range(0, len(a)):\n",
    "    res.append(a[i] * b[i])\n",
    "print(res)\n",
    "\n",
    "res = []\n",
    "for i,j in zip(a,b):\n",
    "    res.append(i * j)\n",
    "print(res)\n",
    "\n",
    "\n",
    "[i*j for i,j in zip(a,b)]\n",
    "\n",
    "\n",
    "import numpy \n",
    "na = numpy.array(a)\n",
    "nb = numpy.array(b)\n",
    "\n",
    "na * nb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "import pandas\n",
    "sa = pandas.Series([1,2,3,4,5], index = ['a', 'b', 'c', 'd', 'e'])\n",
    "sa['a']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "import pandas as pd\n",
    "df = pd.DataFrame([['frank', 'M', 29], ['mary', 'F', 23], ['tom', 'M', 35], ['ted', 'M', 33], ['jean', 'F', 21], ['lisa', 'F', 20]])\n",
    "#df.info()\n",
    "#print(df)\n",
    "df.columns = ['name', 'gender', 'age']\n",
    "#print(df)\n",
    "\n",
    "#df.loc[[0,1,2] , ['name', 'gender'] ]\n",
    "\n",
    "df['age'].mean()\n",
    "df['age'].max()\n",
    "df['age'].min()\n",
    "df['age'].describe()\n",
    "\n",
    "df['age'][0]\n",
    "\n",
    "df['age'][0:3]\n",
    "\n",
    "\n",
    "df.describe()\n",
    "df.iloc[0]\n",
    "df.iloc[0:3]\n",
    "\n",
    "df['name']\n",
    "\n",
    "df[['name', 'age']]\n",
    "\n",
    "df[df['gender'] == 'M']\n",
    "df.loc[df['gender'] == 'M',  ['name', 'age']  ]\n",
    "\n",
    "df.loc[df['gender'] == 'F',  ['age']  ].mean()\n",
    "df.loc[df['gender'] == 'M',  ['age']  ].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register SqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.sql import SQLContext \n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "df = sqlContext.createDataFrame(row_data)\n",
    "df.registerTempTable(\"ratings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark DataFrame Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "#df.take(5)\n",
    "#df.show(10)\n",
    "df.select('userid', 'rating').groupBy('userid').avg().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Data Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "df.registerTempTable(\"ratings\")\n",
    "ratings_data = sqlContext.sql(\"\"\"\n",
    "     SELECT itemid,avg(rating) as avg_rating  from ratings group by itemid order by avg_rating desc \n",
    "\"\"\")\n",
    "ratings_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use toPandas to Convert Spark DataFrame Back To Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "pandas_df = ratings_data.toPandas()\n",
    "pandas_df.columns = ['itemid', 'sum_rating']\n",
    "pandas_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use rdd to Transform Spark DataFrame Back to RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "ratings_out = ratings_data.rdd.map(lambda p : 'itemid:{} - average rating: {}'.format(p.itemid, p.avg_rating))\n",
    "for ele in ratings_out.take(3):\n",
    "    print(ele)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count DataFrame Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "ratings_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)]) \n",
    "y = sc.parallelize([(\"a\", 2), (\"a\", 3)])\n",
    "z = x.join(y)\n",
    "res = z.collect()\n",
    "sorted(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)]) \n",
    "y = sc.parallelize([(\"a\", 2)]) \n",
    "sorted(x.leftOuterJoin(y).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2)]) \n",
    "sorted(y.rightOuterJoin(x).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2), (\"c\", 8)]) \n",
    "sorted(x.fullOuterJoin(y).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table Join By PySpark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.sql import SQLContext \n",
    "from pyspark.sql import Row\n",
    "\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "ratings = sc.textFile('file:/tmp/u.data', 4)\n",
    "ratings_data = ratings.map(lambda l:l.split())\n",
    "ratings_row_data = ratings_data.map(lambda p: \n",
    "    Row( userid=p[0], movieid=p[1], rating=int(p[2]) )\n",
    ")\n",
    "ratings_row_data.take(4)\n",
    "\n",
    "df = sqlContext.createDataFrame(ratings_row_data)\n",
    "df.registerTempTable(\"ratings\")\n",
    "\n",
    "movies = sc.textFile('file:/tmp/u.item', 4)\n",
    "\n",
    "movies_data = movies.map(lambda l:l.split('|'))\n",
    "#movies_data.take(3)\n",
    "movies_row_data = movies_data.map(lambda p: \n",
    "    Row(movieid=p[0], moviename=p[1] )\n",
    ")\n",
    "movies_row_data.take(4)\n",
    "\n",
    "ratings_df = sqlContext.createDataFrame(ratings_row_data)\n",
    "ratings_df.registerTempTable(\"ratings\")\n",
    "\n",
    "movies_df = sqlContext.createDataFrame(movies_row_data)\n",
    "movies_df.registerTempTable(\"movies\")\n",
    "\n",
    "best_movies = sqlContext.sql(\"\"\"\n",
    "     SELECT moviename,avg(rating) as avg_rating, count(1) as cnt  from movies inner join ratings on ratings.movieid = movies.movieid group by moviename order by  avg(rating) desc limit 10\n",
    "\"\"\")\n",
    "best_movies.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "m = ratings_df.join(movies_df, movies_df.movieid == ratings_df.movieid) \\\n",
    "  .groupBy(movies_df.moviename).agg({\"rating\": \"avg\"})\n",
    "m.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
